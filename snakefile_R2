## BEFORE RUNNING: make sure you've activated the conda environment containing all QIseq pipeline dependencies first, like so:
##	module load hub.apps/anaconda/2022.12
##	conda activate QISeq_Deps_Snakemake7
import os
import glob

####################### USER INPUTS ###########################
# provide directory path to your fastq.gz files:
experiment_name="PilotSangerNewStrategy" 
SampleDir = "FASTQs"
	## note that your fastq filenames must end in "_R1.fastq.gz" and "_R2.fastq.gz"

# provide directory path to your reference genome:
RefDir = "RefGenome"
	# RefDir = "/shares/omicshub/References/Plasmodium/Pfalciparum/nf54"

# provide complete path to reference genome fasta file:
#refFasta = RefDir+"/PlasmoDB-56_PknowlesiA1H1_Genome.fasta"
refFasta = RefDir+"/PfNF54.fasta"

# set offset parameter depending on sequencing platform. Default is -2 for NovaSeq
# NovaSeq (both NovaSeqX and NovaSeq6000):
Offset=-2

## HiSeq:
#Offset=4

## MiSeq (old platform; not sure about newest one):
#Offset=5
###############################################################


#ScriptsDir = "/shares/pi_ja2/JG/NewQISeqWorkflows/Updated_QISeq_Helper_Scripts"
insertSize=1000 ##Thomas said he thought was 1,000
Min_Unique=1 ##Minimum number of unique insertions
merge_distance=3 ##Merge predicted insertion sites if within merge_distance bp
platform="Illumina"

determineStartsite_exec="TranslatedPythonHelperScripts/tradis.determineStartsite.NovaSeq.py"
fixbases_exec="TranslatedPythonHelperScripts/tradis_fixbases.py"
fixbases_normalized_correctDup_exec="TranslatedPythonHelperScripts/tradis_fixbases_normalized_correctDup.py"
combine_tables_exec="TranslatedPythonHelperScripts/mergeToTable_copilot_refactored.py"
def get_ref_genome_name(inputpath):
    file_base_name = os.path.basename(inputpath)
    genome_name = ".".join(file_base_name.split(".")[0:-1])
    return genome_name


def get_sample_names(indir):
    fastqfiles=glob.glob(indir.rstrip("/")+"/*R1.fastq.gz")
    sample_names=[]

    for fastq in fastqfiles:
        fastq_basename=os.path.basename(fastq)
        sample_name=fastq_basename.replace(".fastq.gz","")
        sample_name="_".join(sample_name.split("_")[0:-1])
        sample_names.append(sample_name)

    return(sample_names)


##The get read group function may need to be modified for the example data
##and/or the actual data.
##Note for QISeq the lanes are not typically analyzed separately so the lane
##information returned will be arbitary. This is likely why the original 
##workflow used a roound about why to add the sample name to the readgroup to
##avoid having to include the other values required by picard AddOrReplaceReadGroups.
##If using picard for this step substantially improves the runtimes may want to
##just set the other values to "NULL" to avoid giving the impression the values
#are meaningful 
def get_read_group_data(wildcards):
    from gzip import open as gzopen
    sample = wildcards.sample.strip()
    fastq_path="FASTQs/"+sample+"_R1.fastq.gz"
    #fastq_path = f"Trimmed_FASTQs/{sample}_L001_R1_001_val_1.fq.gz"
    with gzopen(fastq_path, 'rt') as f:
        for line in f:
            if line.startswith('@'):
                parts = line.strip().split(':')
                if len(parts) >= 4:
                    return {
                        'flowcell': parts[2],
                        'lane': parts[3]
                    }
    raise ValueError("No valid read header found.")

genomeName = get_ref_genome_name(refFasta)

#samples = ["MP_IC0_F1_D10", "MP_IC10_F2_D10"]
samples=set(get_sample_names(SampleDir))

rule all:
    input:
        expand("FinalCounts/{sample}",sample=samples),
        expand("Insertion_Counts/{sample}.txt",sample=samples),
        expand("SamtoolStats/{sample}_stats.txt",sample=samples)

rule delete_adapter_from_readsR:
    """Remove transposon specific sequence from the forward read"""
    ##This rule is a low priority to prevent the accumulation of large temporary files
    priority: -1
    input:
        SampleDir+"/{sample}_R2.fastq.gz",
    output:
        temp("TrimmedR2/{sample}_R2_adapter_trimmed.fastq.gz"),
    threads: 20
    resources:
        runtime=60
    params:
        "TrimmedR2/{sample}_R2_adapter_trimmed.fastq",
    ##This shell command removes the first 5 bases from the sequence and quality lines of each read.
    ##This is the sequence 5' to the TTAA site and in the current protocol is TAGGG.
    ##If the library prep protocol changes this may need to be updated.
    shell:
        """zcat {input} | perl -e 'while (<>){{ print; $_=<STDIN>; print substr($_,6);$_=<STDIN>;print; $_=<STDIN>; print substr($_,6); }}' > {params} && pigz -p {threads} {params}"""

rule create_bowtie_index:
    """Creates the bowtie index in RefDir if it doesn't already exist"""
    priority: 1
    input:
        refFasta

    output:
        RefDir+"/"+genomeName+".1.bt2"

    shell:
        "bowtie2-build {input} {RefDir}/{genomeName}"

rule samtools_index_genome:
    """Creates a samtools index of the reference genome if it already doesn't exist"""
    priority: 1
    input:
        refFasta

    output:
        refFasta+".fai"

    shell:
        "samtools faidx {input}"

rule bowtie_align:
    """Align the R1 adapter trimmed and R2 fastqs to the reference genome"""
    ##This rule is a low priority to prevent the accumulation of large temporary files.
    ##May want to adjust the number of threads and runtime to your system.
    priority: -1
    input:
        R1=SampleDir+"/{sample}_R1.fastq.gz",
        R2="TrimmedR2/{sample}_R2_adapter_trimmed.fastq.gz",
        ref_genome= RefDir+"/"+genomeName+".1.bt2"

    output:
        temp("BowtieSams/{sample}.sam")

    threads: 20

    resources:
        runtime=120

    ##--very-sensitive is a preset option to set default values for parameters: D,R,N,L,and i.
    ##-N is the number of mismatches in a seed allowed and is modified from the preset to make it less strict
    ##-L is the length of the seed and is set higher than the preset.
    ##--rdg is the read gap open and extend penalty. The extend penalty is lower than the default.

    shell:
        "bowtie2 -X {insertSize} -p {threads} --very-sensitive -N 1 -L 31 --rdg 5,2 -x {RefDir}/{genomeName} -1 {input.R1} -2 {input.R2} -S {output}"


##It may be more efficient to split this into 2 rules.
##Also it may not even be neccessary to add the read group info
rule alignment_initial_processing:
    """Sorts, adds sampleID read group and converts to bam file"""

    priority: 1

    input:
        sam_file="BowtieSams/{sample}.sam",
        genomeIndex=refFasta+".fai"

    output:
        temp("BowtieBams/{sample}.bam")

    resources:
        runtime=90

    threads: 20

    shell:
        "samtools sort -@ {threads}  {input.sam_file} | samtools addreplacerg -  -m overwrite_all -r 'ID:1' -r 'SM:{wildcards.sample}' --reference {input.genomeIndex} -O BAM -o {output}"

##Maybe better to replace this with a rule that does samtools sort | samtools addreplacerg
##that way don't need to specify the unneeded readgroup info
##This is less efficient than the original implementation
#rule alignment_initial_processing:
#    """Sorts and converts alignments to bam. Adds sampleID read group information"""
#    priority: 1
#    input:
#        "BowtieSams/{sample}.sam"
#
#    output:
#        temp("BowtieBams/{sample}.bam")
#
#    resources:
#        runtime=120
#
#    params:
#        flowcell=lambda wildcards: get_read_group_data(wildcards)["flowcell"],
#        lane=lambda wildcards: get_read_group_data(wildcards)["lane"]
#
#    shell:
#        "picard AddOrReplaceReadGroups -I {input} -O {output} -SORT_ORDER coordinate -LB {wildcards.sample} -PL {platform} -PU {params.flowcell}.{params.lane}.{wildcards.sample} -SM {wildcards.sample}"
        


rule mark_duplicates:
    """Mark the duplicate reads. Required to count the number of unique reads associated with a site."""
    priority: 5
    input:
        "BowtieBams/{sample}.bam"
        ##These are not temporary
    output:
        "MarkDups_Bams/{sample}.bam"
    resources:
        runtime=60,
        mem_mb=lambda wildcards, input: max(2*input.size_mb,4000)
	## JO upped max mem from 2000 to 4000

    shell:
        "picard MarkDuplicates VALIDATION_STRINGENCY=SILENT M=/dev/null ASSUME_SORTED=TRUE MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000 I={input} O={output}"


rule index_bams:
    """Index the bam files. Not required QISeq, but useful for downstream analysis"""
    priority: 1
    input:
        "MarkDups_Bams/{sample}.bam"

    output:
        "MarkDups_Bams/{sample}.bai"

    shell:
        "samtools index {input} {output}"


rule count_insertions_exclude_duplicates:
    """Counts the number of unique reads mapped to each site"""
    priority: 1
    input:
        bam="MarkDups_Bams/{sample}.bam",
        ##This is just to make sure index gets made
        index="MarkDups_Bams/{sample}.bai"
    output:
        "Insertion_Counts_No_Dups/{sample}.txt"

    ##-f 66 selects for R1 reads that are properly paired. If the library prep protocol changes this may need to change.
    ## -F 1024 excludes reads that are marked as duplicates.
    ##The awk commands excludes reads with soft clipping (CIGAR contains S). Prior versions of the workflow did not exclude reads with soft clipping.
    ##determineStartsite_exec calculates the effective length of the alignment which is required for determinging the start site of reads mapped to the reverse strand.
    ##| sort | uniq -c | sort -rn counts the number of unique insertions at each site and sorts in descending order.
    ##fixbases_exec is a merging & filtering step. Keeps the first site encountered and merges other sites within 10 bp into that site. Filters results by minimum unique count.
    ##Note: the subsequent rule combine_count_tables has the option to further merge the sites.

    shell:
        """samtools view -f 130 -F 1024 {input.bam} | awk ' !($6 ~ "S")' | {determineStartsite_exec} {Offset} | sort | uniq -c |sort -rn | {fixbases_exec} {Min_Unique} > {output}"""


rule count_insertions:
    """Sums the number of reads that map to each of the previously identified sites"""
    ##This rule is given the highest priority so that large temporary files may be cleared out quickly
    priority: 10
    input:
        bam="MarkDups_Bams/{sample}.bam",
        ##This is just to make sure index gets made
        index="MarkDups_Bams/{sample}.bai",
        unique_count="Insertion_Counts_No_Dups/{sample}.txt"

    output:
        "Insertion_Counts/{sample}.txt"
    ##-f 66 selects for R1 reads that are properly paired. If the library prep protocol changes this may need to change.
    ##determineStartsite_exec calculate the effective lenght of the alignment to determine the start site. Note soft clipped reads are no longer excluded.
    ##| sort | uniq -c | sort -rn counts the number of unique insertions at each site and sorts in descending order.
    ##fixbases_normalized_correctDup_exec bins and sums the alignment counts cooresponding to the "ok" sites identified by fixbases_exec in the prior rule.
    ##fixbases_normalized_correctDup_exec also performs a relative abundance normalization, but this not currently recommended for any downstream analysis.

    shell:
        """samtools view -f 130 {input.bam} | {determineStartsite_exec} {Offset} |  sort | uniq -c |sort -rn | {fixbases_normalized_correctDup_exec}  {input.unique_count}  > {output}"""

rule run_samtools_stats:
    priority: 1
    input:
        bam="MarkDups_Bams/{sample}.bam",
        index="MarkDups_Bams/{sample}.bai"

    output:
        "SamtoolStats/{sample}_stats.txt"
    threads: 10
    resources:
        runtime=10,
        mem_mb=1000
    shell:
        "samtools stats -@ {threads} {input.bam} > {output}"


rule combine_count_tables:
    """Combine the counts for the different samples merging sites within merge_distance"""
    priority: 1
    
    input:
        expand("Insertion_Counts/{sample}.txt",sample=samples)

    output:
        temp(touch(expand("FinalCounts/{sample}",sample=samples)))

    params:
        "Insertion_Counts"

    shell:
        "{combine_tables_exec} {params} {merge_distance} > FinalCounts/{experiment_name}_insertion_counts.tsv"
